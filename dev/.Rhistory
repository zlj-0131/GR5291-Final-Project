# Generate new dataset
column_keep <- c("HEALTH", selected_features)
new_df <- df[column_keep]
head(new_df)
# Export to csv
write.csv(new_df,"D:/Columbia University/GR5291-Final-Project/data/Feature_selection_data.csv", row.names = FALSE)
# Determine 20 most importanct features
selected_features <- vi$features[1:20]
# Generate new dataset
column_keep <- c("HEALTH", selected_features)
new_df <- df[column_keep]
head(new_df)
# Export to csv
write.csv(new_df,"D:/Columbia University/GR 5291/GR5291-Final-Project/data/Feature_selection_data.csv", row.names = FALSE)
# calculate correlation matrix
correlationMatrix <- cor(new_df)
# find attributes that are highly corrected (ideally >0.75)
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.5)
# print indexes of highly correlated attributes
print(highlyCorrelated)
# Construct a heatmap of the dataset
heatmap(as.matrix(new_df), Colv = NA, Rowv = NA, scale="column")
# calculate correlation matrix
correlationMatrix <- cor(new_df)
# find attributes that are highly corrected (ideally >0.75)
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.5)
# print indexes of highly correlated attributes
print(highlyCorrelated)
# Construct a heatmap of the dataset
heatmap(new_df, Colv = NA, Rowv = NA, scale="column")
# calculate correlation matrix
correlationMatrix <- cor(new_df)
# find attributes that are highly corrected (ideally >0.75)
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.5)
# print indexes of highly correlated attributes
print(highlyCorrelated)
# Construct a heatmap of the dataset
heatmap(as.matrix(new_df), Colv = NA, Rowv = NA, scale="column")
# calculate correlation matrix
correlationMatrix <- cor(new_df)
# find attributes that are highly corrected (ideally >0.75)
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.5)
# print indexes of highly correlated attributes
print(highlyCorrelated)
# Construct a heatmap of the dataset
heatmap(as.matrix(new_df), scale="column")
library(reshape2)
# calculate correlation matrix
correlationMatrix <- cor(new_df)
# find attributes that are highly corrected (ideally >0.75)
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.5)
# print indexes of highly correlated attributes
print(highlyCorrelated)
cormat <- round(correlationMatrix,2)
head(cormat)
melted_cormat <- melt(cormat)
head(melted_cormat)
# Construct a heatmap of the dataset
ggplot(data = melted_cormat, aes(x=Var1, y=Var2, fill=value)) +
geom_tile()
# calculate correlation matrix
correlationMatrix <- cor(new_df)
# find attributes that are highly corrected (ideally >0.75)
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.5)
# print indexes of highly correlated attributes
print(highlyCorrelated)
cormat <- round(correlationMatrix,2)
head(cormat)
melted_cormat <- melt(cormat)
head(melted_cormat)
# Construct a heatmap of the dataset
ggheatmap <- ggplot(melted_cormat, aes(Var2, Var1, fill = value))+
geom_tile(color = "white")+
scale_fill_gradient2(low = "blue", high = "red", mid = "white",
midpoint = 0, limit = c(-1,1), space = "Lab",
name="Pearson\nCorrelation") +
theme_minimal()+ # minimal theme
theme(axis.text.x = element_text(angle = 45, vjust = 1,
size = 12, hjust = 1))+
coord_fixed()
# Print the heatmap
print(ggheatmap)
# calculate correlation matrix
correlationMatrix <- cor(new_df)
# find attributes that are highly corrected (ideally >0.75)
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.5)
# print indexes of highly correlated attributes
print(highlyCorrelated)
cormat <- round(correlationMatrix,2)
# Reorder the correlation matrix
cormat <- reorder_cormat(cormat)
# calculate correlation matrix
correlationMatrix <- cor(new_df)
# find attributes that are highly corrected (ideally >0.75)
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.5)
# print indexes of highly correlated attributes
print(highlyCorrelated)
cormat <- round(correlationMatrix,2)
# reorder the correlation matrix helper function
reorder_cormat <- function(cormat){
# Use correlation between variables as distance
dd <- as.dist((1-cormat)/2)
hc <- hclust(dd)
cormat <- cormat[hc$order, hc$order]
}
# Reorder the correlation matrix
cormat <- reorder_cormat(cormat)
upper_tri <- get_upper_tri(cormat)
# calculate correlation matrix
correlationMatrix <- cor(new_df)
# find attributes that are highly corrected (ideally >0.75)
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.5)
# print indexes of highly correlated attributes
print(highlyCorrelated)
cormat <- round(correlationMatrix,2)
# reorder the correlation matrix helper function
reorder_cormat <- function(cormat){
# Use correlation between variables as distance
dd <- as.dist((1-cormat)/2)
hc <- hclust(dd)
cormat <- cormat[hc$order, hc$order]
}
# Get lower triangle of the correlation matrix
get_lower_tri<-function(cormat){
cormat[upper.tri(cormat)] <- NA
return(cormat)
}
# Get upper triangle of the correlation matrix
get_upper_tri <- function(cormat){
cormat[lower.tri(cormat)]<- NA
return(cormat)
}
# Reorder the correlation matrix
cormat <- reorder_cormat(cormat)
upper_tri <- get_upper_tri(cormat)
# Melt the correlation matrix
melted_cormat <- melt(upper_tri, na.rm = TRUE)
# Construct a heatmap of the dataset
ggheatmap <- ggplot(melted_cormat, aes(Var2, Var1, fill = value))+
geom_tile(color = "white")+
scale_fill_gradient2(low = "blue", high = "red", mid = "white",
midpoint = 0, limit = c(-1,1), space = "Lab",
name="Pearson\nCorrelation") +
theme_minimal()+ # minimal theme
theme(axis.text.x = element_text(angle = 45, vjust = 1,
size = 12, hjust = 1))+
coord_fixed()
# Print the heatmap
print(ggheatmap)
knitr::opts_chunk$set(echo = TRUE)
inpt = blanced_df[, -which(colnames(blanced_df) == "HEALTH")]
new_df <- new_df %>%
mutate(HEALTH = as.factor(HEALTH))
# Load libraries
library(tidyr)
library(tidyverse)
library(dplyr)
library(mlbench)
library(ggplot2)
library(randomForest)
library(caret)
library(reshape2)
library(bnstruct)
# Load the dataset
df <- read.csv("../data/NFWBS_PUF_2016_data.csv")
head(df)
# Show dimension of the dataframe
dim(df)
# Set PUF_ID as index
df <- df %>%
remove_rownames %>%
column_to_rownames(var="PUF_ID")
# notice that negative values are invalid entries,
# so replacing them with NA
for (i in 1:nrow(df)){
for (j in 1:ncol(df)){
if (df[i,j] < 0){
df[i,j] = NA
}
}
}
# number of na values in row and column
nacol = colSums(is.na(df))
mean(nacol); sd(nacol)
narow = rowSums(is.na(df))
mean(narow); sd(narow)
# use knn impute to resolve NA problem
df = knn.impute(as.matrix(df)) %>%
as.data.frame()
head(df)
# Check NA values
colSums(is.na(df))
df <- df %>%
mutate(HEALTH = ifelse(HEALTH == -1 | HEALTH == 1 | HEALTH == 2, 0, 1))
# Fit random forest to the dataset
fit <- randomForest(HEALTH~., data=df)
# Show the variable importance table
vi <- varImp(fit, scale = FALSE)
vi <- rownames_to_column(vi, "features")
vi[order(-vi$Overall),]
# Plot the variable importance
varImpPlot(fit,type=2)
# Determine 20 most importanct features
selected_features <- vi$features[1:20]
# Generate new dataset
column_keep <- c("HEALTH", selected_features)
new_df <- df[column_keep]
head(new_df)
# Export to csv
write.csv(new_df,"../data/Feature_selection_data.csv", row.names = FALSE)
# calculate correlation matrix
correlationMatrix <- cor(new_df)
# find attributes that are highly corrected (ideally >0.75)
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.75)
# print indexes of highly correlated attributes
print(highlyCorrelated)
cormat <- round(correlationMatrix,2)
# reorder the correlation matrix helper function
reorder_cormat <- function(cormat){
# Use correlation between variables as distance
dd <- as.dist((1-cormat)/2)
hc <- hclust(dd)
cormat <- cormat[hc$order, hc$order]
}
# Get lower triangle of the correlation matrix
get_lower_tri<-function(cormat){
cormat[upper.tri(cormat)] <- NA
return(cormat)
}
# Get upper triangle of the correlation matrix
get_upper_tri <- function(cormat){
cormat[lower.tri(cormat)]<- NA
return(cormat)
}
# Reorder the correlation matrix
cormat <- reorder_cormat(cormat)
upper_tri <- get_upper_tri(cormat)
# Melt the correlation matrix
melted_cormat <- melt(upper_tri, na.rm = TRUE)
# Construct a heatmap of the dataset
text_size = 2
ggheatmap <- ggplot(melted_cormat, aes(Var2, Var1, fill = value))+
geom_tile(color = "white")+
geom_text(aes(label = round(value, 1)), size = text_size) +
scale_fill_gradient2(low = "blue", high = "red", mid = "white",
midpoint = 0, limit = c(-1,1), space = "Lab",
name="Pearson\nCorrelation") +
theme_minimal()+ # minimal theme
theme(axis.text.x = element_text(angle = 90, vjust = 1, hjust = 1),
text = element_text(size = text_size * (5))) +
coord_fixed()
# Print the heatmap
print(ggheatmap)
library(lares)
corr_cross(new_df, # name of dataset
max_pvalue = 0.05, # display only significant correlations (at 5% level)
top = 10 # display top 10 couples of variables (by correlation coefficient)
)
library(Information)
library(gridExtra)
### Ranking variables using penalized IV
IV <- create_infotables(data = new_df, y = "HEALTH", bins=10, parallel=TRUE)
IV_Value = data.frame(IV$Summary)
IV_Value
new_df <- subset(new_df, select = -c(FS1_1, FWB2_1, FWB1_3))
dim(new_df)
# Export to csv
write.csv(new_df, "../data/Feature_selection_data.csv", row.names = FALSE)
# Imbalance Data
new_df <- new_df %>%
mutate(HEALTH = as.factor(HEALTH))
as.data.frame(table(new_df$HEALTH))
ggplot(new_df , aes(x = factor(HEALTH), fill = factor(HEALTH))) +
geom_bar() +
xlab("Healthy OR Not") +
labs(fill="Healthy OR Not")
new_df <- new_df %>%
mutate(HEALTH = as.factor(HEALTH))
# SMOTE : : Synthetic Minority Oversampling Technique To Handle Class Imbalancy In Binary Classification
library(DMwR)
balanced_df <- SMOTE(HEALTH~., new_df, perc.over = 500, perc.under = 115, k = 5)
as.data.frame(table(balanced_df$HEALTH))
ggplot(balanced_df , aes(x = factor(HEALTH), fill = factor(HEALTH))) +
geom_bar() +
xlab("Healthy OR Not") +
labs(fill="Healthy OR Not")
# Export to csv
write.csv(balanced_df, "../data/balance_data.csv", row.names = FALSE)
inpt = blanced_df[, -which(colnames(blanced_df) == "HEALTH")]
inpt = balanced_df[, -which(colnames(balanced_df) == "HEALTH")]
resp = balanced_df$HEALTH
# separation of train and test data
testind = sample(1:nrow(balanced_df), round(nrow(balanced_df) * 0.2), replace = F)
train.x = inpt[-testind, ]
test.x = inpt[testind, ]
train.y = resp[-testind]
test.y = resp[testind]
# train LASSO regression
cv <- cv.glmnet(x = as.matrix(train.x), y = as.double(train.y),
family="gaussian", alpha = 1, intercept=TRUE)
# Load libraries
library(tidyr)
library(tidyverse)
library(dplyr)
library(mlbench)
library(ggplot2)
library(randomForest)
library(caret)
library(reshape2)
library(bnstruct)
library(glmnet)
# Load the dataset
df <- read.csv("../data/NFWBS_PUF_2016_data.csv")
head(df)
inpt = balanced_df[, -which(colnames(balanced_df) == "HEALTH")]
resp = balanced_df$HEALTH
# separation of train and test data
testind = sample(1:nrow(balanced_df), round(nrow(balanced_df) * 0.2), replace = F)
train.x = inpt[-testind, ]
test.x = inpt[testind, ]
train.y = resp[-testind]
test.y = resp[testind]
# train LASSO regression
cv <- cv.glmnet(x = as.matrix(train.x), y = as.double(train.y),
family="gaussian", alpha = 1, intercept=TRUE)
bestlambda <- cv$lambda.min
lasso <- glmnet(x = as.matrix(train.x), y = as.double(train.y),
family="gaussian", alpha = 1, intercept=TRUE, lambda = bestlambda)
summary(lasso)
coef = rbind("(intercept)" = lasso$a0, as.data.frame(as.matrix(lasso$beta))) %>%
dplyr::arrange(desc(s0)) %>% filter(s0 != 0)
coef
# Show the variable importance table
importance = dplyr::arrange(coef, desc(abs(s0)))
importance
# select top 20 important variables
selectedvar = rownames(importance)[2:21]
pred = predict(lasso, newx = as.matrix(test.x))
mean((test.y - pred)^2)
confusionMatrix(factor(round(pred)), factor(test.y))
pred = predict(lasso, newx = as.matrix(test.x))
mean((test.y - pred)^2)
confusionMatrix(factor(round(pred)), test.y)
pred = predict(lasso, newx = as.matrix(test.x))
mean((test.y - pred)^2)
confusionMatrix(factor(round(pred)), test.y)
pred = predict(lasso, newx = as.matrix(test.x))
mean((test.y - pred)^2)
confusionMatrix(round(pred), test.y)
pred
range(pred)
# Load libraries
library(tidyr)
library(tidyverse)
library(dplyr)
library(mlbench)
library(ggplot2)
library(randomForest)
library(caret)
library(reshape2)
library(bnstruct)
library(glmnet)
# Load the dataset
df <- read.csv("../data/NFWBS_PUF_2016_data.csv")
head(df)
# Show dimension of the dataframe
dim(df)
# Set PUF_ID as index
df <- df %>%
remove_rownames %>%
column_to_rownames(var="PUF_ID")
# notice that negative values are invalid entries,
# so replacing them with NA
for (i in 1:nrow(df)){
for (j in 1:ncol(df)){
if (df[i,j] < 0){
df[i,j] = NA
}
}
}
# number of na values in row and column
nacol = colSums(is.na(df))
mean(nacol); sd(nacol)
narow = rowSums(is.na(df))
mean(narow); sd(narow)
# use knn impute to resolve NA problem
df = knn.impute(as.matrix(df)) %>%
as.data.frame()
head(df)
# Check NA values
colSums(is.na(df))
df <- df %>%
mutate(HEALTH = ifelse(HEALTH == -1 | HEALTH == 1 | HEALTH == 2, 0, 1))
# Fit random forest to the dataset
fit <- randomForest(HEALTH~., data=df)
# Show the variable importance table
vi <- varImp(fit, scale = FALSE)
vi <- rownames_to_column(vi, "features")
vi[order(-vi$Overall),]
# Plot the variable importance
varImpPlot(fit,type=2)
# Determine 20 most importanct features
selected_features <- vi$features[1:20]
# Generate new dataset
column_keep <- c("HEALTH", selected_features)
new_df <- df[column_keep]
head(new_df)
# Export to csv
write.csv(new_df,"../data/Feature_selection_data.csv", row.names = FALSE)
# calculate correlation matrix
correlationMatrix <- cor(new_df)
# find attributes that are highly corrected (ideally >0.75)
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.75)
# print indexes of highly correlated attributes
print(highlyCorrelated)
cormat <- round(correlationMatrix,2)
# reorder the correlation matrix helper function
reorder_cormat <- function(cormat){
# Use correlation between variables as distance
dd <- as.dist((1-cormat)/2)
hc <- hclust(dd)
cormat <- cormat[hc$order, hc$order]
}
# Get lower triangle of the correlation matrix
get_lower_tri<-function(cormat){
cormat[upper.tri(cormat)] <- NA
return(cormat)
}
# Get upper triangle of the correlation matrix
get_upper_tri <- function(cormat){
cormat[lower.tri(cormat)]<- NA
return(cormat)
}
# Reorder the correlation matrix
cormat <- reorder_cormat(cormat)
upper_tri <- get_upper_tri(cormat)
# Melt the correlation matrix
melted_cormat <- melt(upper_tri, na.rm = TRUE)
# Construct a heatmap of the dataset
text_size = 2
ggheatmap <- ggplot(melted_cormat, aes(Var2, Var1, fill = value))+
geom_tile(color = "white")+
geom_text(aes(label = round(value, 1)), size = text_size) +
scale_fill_gradient2(low = "blue", high = "red", mid = "white",
midpoint = 0, limit = c(-1,1), space = "Lab",
name="Pearson\nCorrelation") +
theme_minimal()+ # minimal theme
theme(axis.text.x = element_text(angle = 90, vjust = 1, hjust = 1),
text = element_text(size = text_size * (5))) +
coord_fixed()
# Print the heatmap
print(ggheatmap)
library(lares)
corr_cross(new_df, # name of dataset
max_pvalue = 0.05, # display only significant correlations (at 5% level)
top = 10 # display top 10 couples of variables (by correlation coefficient)
)
library(Information)
library(gridExtra)
### Ranking variables using penalized IV
IV <- create_infotables(data = new_df, y = "HEALTH", bins=10, parallel=TRUE)
IV_Value = data.frame(IV$Summary)
IV_Value
new_df <- subset(new_df, select = -c(FS1_1, FWB2_1, FWB1_3))
dim(new_df)
# Export to csv
write.csv(new_df, "../data/Feature_selection_data.csv", row.names = FALSE)
new_df <- new_df %>%
mutate(HEALTH = as.factor(HEALTH))
# Train / Test dataset split
## 75% of the sample size
smp_size <- floor(0.75 * nrow(new_df))
## set the seed to make your partition reproducible
set.seed(123)
train_ind <- sample(seq_len(nrow(new_df)), size = smp_size)
train_df <- mtcars[train_ind, ]
test_df <- mtcars[-train_ind, ]
# Export to csv
write.csv(train_df, "../data/train_data.csv", row.names = FALSE)
write.csv(test_df, "../data/test_data.csv", row.names = FALSE)
View(test_df)
View(train_df)
View(test.x)
new_df <- new_df %>%
mutate(HEALTH = as.factor(HEALTH))
# Train / Test dataset split
## 75% of the sample size
smp_size <- floor(0.75 * nrow(new_df))
## set the seed to make your partition reproducible
set.seed(123)
train_ind <- sample(seq_len(nrow(new_df)), size = smp_size)
train_df <- new_df[train_ind, ]
test_df <- new_df[-train_ind, ]
# Export to csv
write.csv(train_df, "../data/train_data.csv", row.names = FALSE)
write.csv(test_df, "../data/test_data.csv", row.names = FALSE)
# Imbalance Data
as.data.frame(table(train_df$HEALTH))
ggplot(train_df , aes(x = factor(HEALTH), fill = factor(HEALTH))) +
geom_bar() +
xlab("Healthy OR Not") +
labs(fill="Healthy OR Not")
# SMOTE : : Synthetic Minority Oversampling Technique To Handle Class Imbalancy In Binary Classification
library(DMwR)
train_balanced_df <- SMOTE(HEALTH~., new_df, perc.over = 500, perc.under = 115, k = 5)
as.data.frame(table(balanced_df$HEALTH))
ggplot(train_balanced_df , aes(x = factor(HEALTH), fill = factor(HEALTH))) +
geom_bar() +
xlab("Healthy OR Not") +
labs(fill="Healthy OR Not")
# Export to csv
write.csv(balanced_df, "../data/train_balanced_data.csv", row.names = FALSE)
